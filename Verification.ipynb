{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s.ezati\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from imutils import face_utils\n",
    "from collections import Counter\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "import dlib\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from Denoiser.GFPGANer import GFPGANer_Model\n",
    "from retinaface import RetinaFace\n",
    "from deepface import DeepFace\n",
    "from deepface.commons import distance\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "frame_x = 1250 #1250\n",
    "frame_y = 750 #750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser = GFPGANer_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_colors = []\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_square(img, min_size, fill_color=(200, 200, 200)):\n",
    "    img_x_y =(img.shape[1]/img.shape[0])\n",
    "    img = cv2.resize(np.array(img),(round(min_size*img_x_y),min_size))\n",
    "    im = Image.fromarray(img)\n",
    "    x, y = im.size\n",
    "    size = max(min_size, x, y)\n",
    "    new_im = Image.new('RGB', (size, size), fill_color)\n",
    "    new_im.paste(im, (int((size -x) / 2), int((size -y) / 2)))\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fix_Face(img,left_eye=None,right_eye=None):\n",
    "    gray = img.copy()\n",
    "    \n",
    "    if left_eye is None and right_eye is None:\n",
    "        try: m = mtcnn.detect_faces(gray)\n",
    "        except ValueError: return img\n",
    "\n",
    "        ex = m[0]['keypoints']['right_eye'][0]\n",
    "        ey = m[0]['keypoints']['right_eye'][1]\n",
    "\n",
    "        left_eye_center = (m[0]['keypoints']['left_eye'][0],m[0]['keypoints']['left_eye'][1])\n",
    "        left_eye_x = m[0]['keypoints']['left_eye'][0]\n",
    "        left_eye_y = m[0]['keypoints']['left_eye'][1]\n",
    "\n",
    "        right_eye_center = (m[0]['keypoints']['right_eye'][0],m[0]['keypoints']['right_eye'][1])\n",
    "        right_eye_x = m[0]['keypoints']['right_eye'][0]\n",
    "        right_eye_y = m[0]['keypoints']['right_eye'][1]\n",
    "\n",
    "        delta_x = right_eye_x - left_eye_x\n",
    "        delta_y = right_eye_y - left_eye_y\n",
    "        angle = np.arctan(delta_y/delta_x)\n",
    "        angle = (angle * 180) / np.pi\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, (angle), 1.0)\n",
    "\n",
    "        rotated = cv2.warpAffine(img, M, (w, h))\n",
    "        \n",
    "    else:\n",
    "        left_eye_x = left_eye[0]\n",
    "        left_eye_y = left_eye[1]\n",
    "\n",
    "        right_eye_x = right_eye[0]\n",
    "        right_eye_y = right_eye[1]\n",
    "\n",
    "        delta_x = right_eye_x - left_eye_x\n",
    "        delta_y = right_eye_y - left_eye_y\n",
    "        angle = np.arctan(delta_y/delta_x)\n",
    "        angle = (angle * 180) / np.pi\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, (angle), 1.0)\n",
    "\n",
    "        rotated = cv2.warpAffine(img, M, (w, h))  \n",
    "        \n",
    "    return rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pre_Proc_Light(img):\n",
    "    lightness = np.mean(img)\n",
    "    if lightness < 95:\n",
    "        img_yuv = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "        return cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)\n",
    "    else: return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=datasets.ImageFolder('people/')\n",
    "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} \n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "face_list = [] \n",
    "name_list = [] \n",
    "embedding_list = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n"
     ]
    }
   ],
   "source": [
    "for imgs, idx in loader:\n",
    "    \n",
    "    faces = RetinaFace.detect_faces(np.array(imgs))\n",
    "\n",
    "    for face in faces:\n",
    "\n",
    "        save_flag = True\n",
    "        try: score = faces[face]['score']*100\n",
    "        except TypeError: break\n",
    "\n",
    "        if score < 90: save_flag = False\n",
    "        else: \n",
    "            x = faces[face]['facial_area'][0]\n",
    "            y = faces[face]['facial_area'][1]\n",
    "            w = faces[face]['facial_area'][2]\n",
    "            h = faces[face]['facial_area'][3]\n",
    "\n",
    "            img = np.array(imgs)[y:h, x:w].copy()\n",
    "            w_resize = img.shape[1]\n",
    "            h_resize = img.shape[0]\n",
    "            try:\n",
    "                x_right = round(abs(faces[face]['landmarks']['right_eye'][0]-x)*(300/w_resize))\n",
    "                y_right = round(abs(faces[face]['landmarks']['right_eye'][1]-y)*(300/h_resize))\n",
    "\n",
    "                x_left = round(abs(faces[face]['landmarks']['left_eye'][0]-x)*(300/w_resize))\n",
    "                y_left = round(abs(faces[face]['landmarks']['left_eye'][1]-y)*(300/h_resize))\n",
    "                        except ZeroDivisionError:\n",
    "                z = 0\n",
    "                       \n",
    "            dist = np.linalg.norm((np.array([x_left,y_left])-np.array([x_right,y_right])),ord=2)\n",
    "            \n",
    "            \n",
    "            img_x_y =(img.shape[1]/img.shape[0])\n",
    "#             _, _, restored_img = denoiser.enhance(np.array(img))\n",
    "            restored_img = Fix_Face(np.array(img),[x_left,y_left],[x_right,y_right])\n",
    "            restored_img = cv2.resize(np.array(restored_img),(round(restored_img.shape[0]*img_x_y),restored_img.shape[0]))\n",
    "            img = make_square(restored_img, 300)\n",
    "           \n",
    "    \n",
    "        try: \n",
    "            embedding = DeepFace.represent(np.array(img),detector_backend=None,model_name='Facenet512')\n",
    "            embedding_list.append(embedding) \n",
    "            name_list.append(idx_to_class[idx])\n",
    "        except ValueError: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "imgs_original = np.array(cv2.imread('17.jpg'))\n",
    "imgs_original = cv2.cvtColor(np.array(imgs_original),cv2.COLOR_BGR2RGB)\n",
    "faces = RetinaFace.detect_faces(np.array(cv2.imread('17.jpg')))\n",
    "imgs_list = []\n",
    "for face in faces:\n",
    "\n",
    "    save_flag = True\n",
    "    try: score = faces[face]['score']*100\n",
    "    except TypeError: break\n",
    "\n",
    "    if score < 90: save_flag = False\n",
    "    else: \n",
    "        x = faces[face]['facial_area'][0]\n",
    "        y = faces[face]['facial_area'][1]\n",
    "        w = faces[face]['facial_area'][2]\n",
    "        h = faces[face]['facial_area'][3]\n",
    "        \n",
    "        imgs = imgs_original.copy()      \n",
    "        img = np.array(imgs)[y:h, x:w].copy()\n",
    "        w_resize = img.shape[1]\n",
    "        h_resize = img.shape[0]\n",
    "        img_x_y =(img.shape[1]/img.shape[0])\n",
    "\n",
    "        img = cv2.resize(np.array(img),(round(300),300))\n",
    "        x, y, restored_img = denoiser.enhance(np.array(img),only_center_face=True)\n",
    "        restored_img = cv2.resize(np.array(restored_img),(round(restored_img.shape[0]*img_x_y),restored_img.shape[1]))\n",
    "        restored_img = make_square(restored_img, 200)\n",
    "        imgs_list.append(restored_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    im = cv2.resize(img, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "    row, col = im.shape[:2]\n",
    "    bottom = im[row-2:row, 0:col]\n",
    "    mean = cv2.mean(bottom)[0]\n",
    "    bordersize = 50\n",
    "\n",
    "    top=bordersize\n",
    "    bottom=bordersize\n",
    "    left=bordersize\n",
    "    right=bordersize\n",
    "\n",
    "    border = cv2.copyMakeBorder(\n",
    "        im,\n",
    "        top=top,\n",
    "        bottom=bottom,\n",
    "        left=left,\n",
    "        right=right,\n",
    "        borderType=cv2.BORDER_CONSTANT,\n",
    "        value=[255, 255, 255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class ThreadedCamera(object):\n",
    "    def __init__(self, source='cam:Sayda@0.0.0.0:88/videoMain'):\n",
    "\n",
    "        self.capture = cv2.VideoCapture(f'rtsp://{source}', cv2.CAP_FFMPEG)\n",
    "\n",
    "        self.thread = Thread(target = self.update, args = ())\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "\n",
    "        self.status = False\n",
    "        self.frame  = None\n",
    "\n",
    "    def update(self):\n",
    "        while True:\n",
    "            if self.capture.isOpened(): (self.status, self.frame) = self.capture.read()\n",
    "            else: continue\n",
    "\n",
    "    def grab_frame(self):\n",
    "        if self.status: return self.frame\n",
    "        else: return None \n",
    "    \n",
    "    def end_frame(self):\n",
    "        self.capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recognition(img,img_original,ROI_Region,thresh=1.04):\n",
    "    now_time = datetime.now().strftime(\"%Y_%m_%d---%I_%M_%S---%p\")\n",
    "    img = np.array(Image.fromarray(img))\n",
    "    img = cv2.cvtColor(np.array(img),cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faces = RetinaFace.detect_faces(img)\n",
    "\n",
    "    for face in faces:\n",
    "\n",
    "        save_flag = True\n",
    "        try: score = faces[face]['score']*100\n",
    "        except TypeError: break\n",
    "\n",
    "        if score < 90: save_flag = False\n",
    "        else: \n",
    "            x = faces[face]['facial_area'][0]\n",
    "            y = faces[face]['facial_area'][1]\n",
    "            w = faces[face]['facial_area'][2]\n",
    "            h = faces[face]['facial_area'][3]\n",
    "          \n",
    "            resize_frame = img[y:h, x:w].copy()\n",
    "            w_resize = resize_frame.shape[1]\n",
    "            h_resize = resize_frame.shape[0]\n",
    "            resize_frame = cv2.resize(resize_frame,(300,300))\n",
    "            \n",
    "            x_right = round(abs(faces[face]['landmarks']['right_eye'][0]-x)*(300/w_resize))\n",
    "            y_right = round(abs(faces[face]['landmarks']['right_eye'][1]-y)*(300/h_resize))\n",
    "\n",
    "            x_left = round(abs(faces[face]['landmarks']['left_eye'][0]-x)*(300/w_resize))\n",
    "            y_left = round(abs(faces[face]['landmarks']['left_eye'][1]-y)*(300/h_resize))\n",
    "            \n",
    "            dist = np.linalg.norm((np.array([x_left,y_left])-np.array([x_right,y_right])),ord=2)\n",
    "            \n",
    "            if dist > 120 :\n",
    "                \n",
    "                img_x_y =(resize_frame.shape[1]/resize_frame.shape[0])\n",
    "                _, _, restored_img = denoiser.enhance(np.array(resize_frame))\n",
    "                restored_img = Fix_Face(np.array(restored_img),[x_left,y_left],[x_right,y_right])\n",
    "                restored_img = cv2.resize(np.array(restored_img),(round(restored_img.shape[0]*img_x_y),restored_img.shape[0]))\n",
    "                restored_img = make_square(restored_img, 300)\n",
    "                \n",
    "                try: embedding = DeepFace.represent(np.array(restored_img),detector_backend=None,model_name='Facenet512')\n",
    "                except ValueError: continue\n",
    "\n",
    "                person,person2,flag,candidates,candidates2,person_id = 'Unknown','Unknown',False,[],[],[]\n",
    "                for indx, emb_db in enumerate(embedding_list):\n",
    "                    res = np.linalg.norm((np.array(emb_db)-np.array(embedding)),ord=2)\n",
    " \n",
    "                    if res < (thresh):\n",
    "                        flag = True\n",
    "                        person = name_list[indx]\n",
    "                        candidates.append((res,person))\n",
    "                        candidates2.append(person)\n",
    "                    else: pass\n",
    "                \n",
    "                if len(candidates2) < 2: pass\n",
    "                else: \n",
    "                    b = Counter(candidates2)\n",
    "                    person2 = b.most_common(1)[0][0]\n",
    "\n",
    "                if flag: \n",
    "                    color = (0,255,0)\n",
    "                    person = min(candidates,key = lambda t: t[0])[1]\n",
    "                    print(\"person\")\n",
    "                    print( person)\n",
    "                    person_id.append(person)\n",
    "                else: color = (0,0,255)   \n",
    "\n",
    "                img_original = cv2.rectangle(np.array(img_original), (x+ROI_Region[0], y+ROI_Region[1],abs(w-(x)),abs(h-(y))), color, 2)\n",
    "                img_original = cv2.putText(np.array(img_original),  f'{person}', (x+ROI_Region[0], y+ROI_Region[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, color,2,cv2.LINE_AA)\n",
    "\n",
    "    img_original = cv2.resize(img_original, (frame_x, frame_y))\n",
    "    try: return img_original,img[y:h, x:w]\n",
    "    except UnboundLocalError: return img_original,img_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mouse_click(event, x, y, flags, param):\n",
    "    global ROI_Region\n",
    "    global img_original2\n",
    "    global img_save\n",
    "    \n",
    "    if event == cv2.EVENT_RBUTTONDOWN:\n",
    "        ROI_Region = cv2.selectROI(\"Select Area\",img_original2)\n",
    "        print(ROI_Region)\n",
    "        cv2.destroyAllWindows()\n",
    "        print(ROI_Region)\n",
    "    elif event == cv2.EVENT_MBUTTONDOWN:\n",
    "        ROI_Region = [0,0,0,0]\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI_Region = [0,0,0,0]\n",
    "# ROI_Region1 = [405, 150, 663, 549]\n",
    "# ROI_Region2 = [266, 188, 639, 486]\n",
    "ROI_Region1 = [0, 0, 0, 0]\n",
    "ROI_Region2 = [0, 0,0, 0]\n",
    "#(441, 114, 459, 487) : 121\n",
    "#(396, 138, 483, 541) : 120\n",
    "frame_list,counter = [],0\n",
    "\n",
    "streamer_2 = ThreadedCamera('rtsp:Sayda@0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 294, 104, 133)\n",
      "(67, 294, 104, 133)\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "person\n",
      "Shabnam\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "person\n",
      "mr zamani\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "person\n",
      "Mohammad\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n"
     ]
    }
   ],
   "source": [
    "while True: \n",
    "    img_original2 = streamer_2.grab_frame()\n",
    "\n",
    "    \n",
    "    try: img_original2 = Image.fromarray(img_original2)\n",
    "    except AttributeError: continue\n",
    "       \n",
    "    img_original2 = cv2.resize(np.array(img_original2), (frame_x, frame_y))\n",
    "\n",
    " \n",
    "    img2 = img_original2.copy()\n",
    "    img_save2 = img_original2.copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    try: \n",
    "        if ROI_Region2[0] == ROI_Region2[1] and ROI_Region2[2] == ROI_Region2[3]:\n",
    "            if ROI_Region2[0] == 0 and ROI_Region2[0] == ROI_Region2[3]: pass\n",
    "            else: \n",
    "                img2 = img2[int(ROI_Region2[1]):int(ROI_Region2[1]+ROI_Region2[3]),\n",
    "                            int(ROI_Region2[0]):int(ROI_Region2[0]+ROI_Region2[2])]\n",
    "        else: \n",
    "            img2 = img2[int(ROI_Region2[1]):int(ROI_Region2[1]+ROI_Region2[3]),\n",
    "                        int(ROI_Region2[0]):int(ROI_Region2[0]+ROI_Region2[2])]\n",
    " \n",
    "    except IndexError: pass\n",
    "    \n",
    "    \n",
    "   \n",
    "        \n",
    "    if img_original2 is None: img_res2 = np.zeros([frame_y,frame_x,3],dtype=np.uint8)\n",
    "    else: img_res2,_ = Recognition(img2,img_original2,ROI_Region2,23.56-3.5)\n",
    "        \n",
    "\n",
    "    img_res2 = cv2.resize(img_res2, (frame_x, frame_y))\n",
    "    cv2.imshow('verification', np.array(img_res2))\n",
    "    \n",
    "    cv2.setMouseCallback('verification', mouse_click)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        cv2.destroyAllWindows()\n",
    "        streamer.end_frame()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True: \n",
    "#     img_original2 = streamer_2.grab_frame()\n",
    "\n",
    "    \n",
    "#     try: img_original2 = Image.fromarray(img_original2)\n",
    "#     except AttributeError: continue\n",
    "       \n",
    "#     img_original2 = cv2.resize(np.array(img_original2), (frame_x, frame_y))\n",
    "\n",
    " \n",
    "#     img2 = img_original2.copy()\n",
    "#     img_save2 = img_original2.copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "#     try: \n",
    "#         if ROI_Region[0] == ROI_Region[1] and ROI_Region[2] == ROI_Region[3]:\n",
    "#             if ROI_Region[0] == 0 and ROI_Region[0] == ROI_Region[3]: pass\n",
    "#             else: \n",
    "#                 img2 = img2[int(ROI_Region[1]):int(ROI_Region[1]+ROI_Region[3]),\n",
    "#                             int(ROI_Region[0]):int(ROI_Region[0]+ROI_Region[2])]\n",
    "#         else: \n",
    "#             img2 = img2[int(ROI_Region[1]):int(ROI_Region[1]+ROI_Region[3]),\n",
    "#                         int(ROI_Region[0]):int(ROI_Region[0]+ROI_Region[2])]\n",
    " \n",
    "#     except IndexError: pass\n",
    "       \n",
    "        \n",
    "#     if img_original2 is None: img_res2 = np.zeros([frame_y,frame_x,3],dtype=np.uint8)\n",
    "#     else: img_res2,_ = Recognition(img2,img_original2,ROI_Region,23.56-3.25)\n",
    "        \n",
    "\n",
    "#     img_res2 = cv2.resize(img_res2, (frame_x, frame_y))\n",
    "#     cv2.imshow('verification', np.array(img_res2))\n",
    "    \n",
    "#     cv2.setMouseCallback('verification', mouse_click)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "#         cv2.destroyAllWindows()\n",
    "#         streamer.end_frame()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True: \n",
    "#     img_original1 = streamer_1.grab_frame()\n",
    "#     img_original2 = streamer_2.grab_frame()\n",
    "    \n",
    "#     try: img_original1 = Image.fromarray(img_original1)\n",
    "#     except AttributeError: pass\n",
    "    \n",
    "#     try: img_original2 = Image.fromarray(img_original2)\n",
    "#     except AttributeError: continue\n",
    "       \n",
    "#     img_original1 = cv2.resize(np.array(img_original1), (frame_x, frame_y))\n",
    "#     img_original2 = cv2.resize(np.array(img_original2), (frame_x, frame_y))\n",
    "\n",
    "#     img1 = img_original1.copy()\n",
    "#     img_save1 = img_original1.copy()\n",
    "    \n",
    "#     img2 = img_original2.copy()\n",
    "#     img_save2 = img_original2.copy()\n",
    "    \n",
    "#     try: \n",
    "#         if ROI_Region1[0] == ROI_Region1[1] and ROI_Region1[2] == ROI_Region1[3]:\n",
    "#             if ROI_Region1[0] == 0 and ROI_Region1[0] == ROI_Region1[3]: pass\n",
    "#             else: \n",
    "#                 img1 = img1[int(ROI_Region1[1]):int(ROI_Region1[1]+ROI_Region1[3]),\n",
    "#                             int(ROI_Region1[0]):int(ROI_Region1[0]+ROI_Region1[2])]\n",
    "#         else: \n",
    "#             img1 = img1[int(ROI_Region1[1]):int(ROI_Region1[1]+ROI_Region1[3]),\n",
    "#                         int(ROI_Region1[0]):int(ROI_Region1[0]+ROI_Region1[2])]\n",
    " \n",
    "#     except IndexError: pass\n",
    "    \n",
    "#     try: \n",
    "#         if ROI_Region2[0] == ROI_Region2[1] and ROI_Region2[2] == ROI_Region2[3]:\n",
    "#             if ROI_Region2[0] == 0 and ROI_Region2[0] == ROI_Region2[3]: pass\n",
    "#             else: \n",
    "#                 img2 = img2[int(ROI_Region2[1]):int(ROI_Region2[1]+ROI_Region2[3]),\n",
    "#                             int(ROI_Region2[0]):int(ROI_Region2[0]+ROI_Region2[2])]\n",
    "#         else: \n",
    "#             img2 = img2[int(ROI_Region2[1]):int(ROI_Region2[1]+ROI_Region2[3]),\n",
    "#                         int(ROI_Region2[0]):int(ROI_Region2[0]+ROI_Region2[2])]\n",
    " \n",
    "#     except IndexError: pass\n",
    "    \n",
    "    \n",
    "#     if img_original1 is None: img_res1 = np.zeros([frame_y,1250,3],dtype=np.uint8)\n",
    "#     else: img_res1,_ = Recognition(img1,img_original1,ROI_Region1,23.56-4.5)\n",
    "        \n",
    "#     if img_original2 is None: img_res2 = np.zeros([frame_y,frame_x,3],dtype=np.uint8)\n",
    "#     else: img_res2,_ = Recognition(img2,img_original2,ROI_Region2,23.56-4.7)\n",
    "        \n",
    "\n",
    "#     img_res1 = cv2.resize(img_res1, (frame_x, frame_y))\n",
    "#     img_res2 = cv2.resize(img_res2, (frame_x, frame_y))\n",
    "#     img = np.concatenate((img_res1, img_res2), axis=1)\n",
    "#     img = cv2.resize(img, (frame_x, frame_y))\n",
    "#     cv2.imshow('verification', np.array(img))\n",
    "    \n",
    "#     cv2.setMouseCallback('verification', mouse_click)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "#         cv2.destroyAllWindows()\n",
    "#         streamer.end_frame()\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
